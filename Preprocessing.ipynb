{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC5P49o4qBtg"
      },
      "source": [
        "# Package preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYoucrp1kXAE",
        "outputId": "4dedfd04-c882-47aa-b419-fc9125f52524"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# standard\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualize\n",
        "from tqdm.notebook import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import multiprocessing\n",
        "\n",
        "# system\n",
        "import pickle     ## saving library\n",
        "import os         ## file manager\n",
        "import sys\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import re         ## preprocessing text library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')     # download toolkit for textblob.TextBlob.words\n",
        "\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer     # tranform expanding words of words like attacker, attacked, attacking -> attack\n",
        "st = PorterStemmer()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"isnt\", \"it\", \"its\", \"itself\", \"keep\", \"keeps\", \"kept\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"names\", \"named\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"ok\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"puts\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"sees\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"shows\", \"showed\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "278X_nwJlWsJ"
      },
      "outputs": [],
      "source": [
        "# Working dir\n",
        "work_path = \"./data\"\n",
        "checkpoint_path = work_path + \"checkpoint/\"\n",
        "\n",
        "# create checkpoint path if not exists\n",
        "if not os.path.exists(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olxWkV5fqGl2"
      },
      "source": [
        "# Preprocessing tool "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tYG29RmXkVie"
      },
      "outputs": [],
      "source": [
        "class preprocess_text():\n",
        "    def preprocess_Abstract(self, text):\n",
        "        if type(text) == str:\n",
        "            text = text.lower()\n",
        "            text = re.sub(\"[\\d+]\", \"\", text)\n",
        "            text = re.sub(\"[^a-z]\", \" \", text)\n",
        "\n",
        "            filtered_sentence = [] \n",
        "            for w in text.split(\" \"): \n",
        "                if w not in stop_words and w not in stop:\n",
        "                    filtered_sentence.append(w)\n",
        "            text = \" \".join(filtered_sentence)\n",
        "            text = text.strip()\n",
        "            text = re.sub(\"[ ]{2,}\", \" \", text)\n",
        "        return text\n",
        "\n",
        "    def preprocess_Title(self, text):\n",
        "        return self.preprocess_Abstract(text)\n",
        "\n",
        "    def preprocess_Aims(self, text):\n",
        "        return self.preprocess_Abstract(text)\n",
        "    \n",
        "    def preprocess_Keywords(self, text):\n",
        "        return self.preprocess_Abstract(text)\n",
        "\n",
        "class preprocess_tool():\n",
        "    def __init__(self, tool_preprocessText = preprocess_text()):\n",
        "        self.tool_preprocessText = tool_preprocessText\n",
        "\n",
        "    def get_preprocessed_data(self, dataframe, \n",
        "                              preprocess_columns = ['title', 'abstract', 'keywords'],\n",
        "                              preprocessing_type = ['Title', 'Abstract', 'Keywords'],\n",
        "                              keep_columns = ['itr'],\n",
        "                              n_jobs=4):\n",
        "        '''-Parameters:\n",
        "              preprocess_columns: choosen columns to apply preprocessing method have definded in class preprocess_text() format.\n",
        "              preprocessing_type: preprocessing methods are applied to respective preprocessing columns have definded in class preprocess_text() format.\n",
        "              keep_columns: columns which we do nothing.\n",
        "           -Return:\n",
        "              (pandas DataFrame): data after preprocess\n",
        "        '''########\n",
        "        output_data = pd.DataFrame(columns=preprocessing_type + keep_columns)\n",
        "        output_data[preprocessing_type + keep_columns] = dataframe[preprocess_columns + keep_columns]\n",
        "\n",
        "        if 'Title' in preprocessing_type:\n",
        "            with Pool(n_jobs) as p:\n",
        "                output_data['Title'] = p.map(self.tool_preprocessText.preprocess_Title, output_data['Title'])\n",
        "        if 'Abstract' in preprocessing_type:\n",
        "            with Pool(n_jobs) as p:\n",
        "                output_data['Abstract'] = p.map(self.tool_preprocessText.preprocess_Abstract, output_data['Abstract'])\n",
        "        if 'Keywords' in preprocessing_type:\n",
        "            with Pool(n_jobs) as p:\n",
        "                output_data['Keywords'] = p.map(self.tool_preprocessText.preprocess_Keywords, output_data['Keywords'])\n",
        "        if \"Aims\" in preprocessing_type:\n",
        "            with Pool(n_jobs) as p:\n",
        "                output_data['Aims'] = p.map(self.tool_preprocessText.preprocess_Aims, output_data['Aims'])\n",
        "        return output_data\n",
        "\n",
        "def labelling_data(series, category):\n",
        "    '''-Parameter:\n",
        "          series(pandas Series): Conference distribution of data.\n",
        "          category(Int64Index list): category (do not reset_index of aims_content before using this funtion)\n",
        "        -Return: \n",
        "          (np array): label series for data.\n",
        "    '''########\n",
        "    label = np.zeros(len(series))\n",
        "    for i, j in enumerate(category):\n",
        "        label[series == j] = i\n",
        "    return label.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO1V49KWqTR-"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Er5CDuu9kxf7"
      },
      "outputs": [],
      "source": [
        "tool_preprocess = preprocess_tool(preprocess_text())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XwVcIgnk1T5",
        "outputId": "f8a14194-b43a-49a4-f89f-d61d0ccb3377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing train ....\n"
          ]
        }
      ],
      "source": [
        "print(\"Preprocessing train ....\")\n",
        "data_train = pd.read_csv(work_path + \"/raw_data/data_splited_train.csv\", encoding = \"ISO-8859-1\")\n",
        "data_train = tool_preprocess.get_preprocessed_data(\n",
        "    data_train,\n",
        "    preprocess_columns = [\"title\", \"abstract\", \"keywords\"],\n",
        "    preprocessing_type = [\"Title\", \"Abstract\", \"Keywords\"],\n",
        "    keep_columns = [\"itr\"],\n",
        "    n_jobs=multiprocessing.cpu_count()\n",
        "    )\n",
        "\n",
        "print(\"Preprocessing validate ....\")\n",
        "data_validate = pd.read_csv(work_path + \"/raw_data/data_splited_validate.csv\", encoding = \"ISO-8859-1\")\n",
        "data_validate = tool_preprocess.get_preprocessed_data(\n",
        "    data_validate,\n",
        "    preprocess_columns = [\"title\", \"abstract\", \"keywords\"],\n",
        "    preprocessing_type = [\"Title\", \"Abstract\", \"Keywords\"],\n",
        "    keep_columns = [\"itr\"],\n",
        "    n_jobs=multiprocessing.cpu_count()\n",
        "    )\n",
        "\n",
        "print(\"Preprocessing test ....\")\n",
        "data_test = pd.read_csv(work_path + \"/raw_data/data_origin_test.csv\", encoding = \"ISO-8859-1\")\n",
        "data_test = tool_preprocess.get_preprocessed_data(\n",
        "    data_test,\n",
        "    preprocess_columns = [\"title\", \"abstract\", \"keywords\"],\n",
        "    preprocessing_type = [\"Title\", \"Abstract\", \"Keywords\"],\n",
        "    keep_columns = [\"itr\"],\n",
        "    n_jobs=multiprocessing.cpu_count()\n",
        "    )\n",
        "\n",
        "print(\"Preprocessing aims ....\")\n",
        "data_aims = pd.read_csv(work_path + \"/raw_data/aims_scopes.csv\", encoding = \"ISO-8859-1\")\n",
        "data_aims = tool_preprocess.get_preprocessed_data(\n",
        "    data_aims,\n",
        "    preprocess_columns = [\"aims\"],\n",
        "    preprocessing_type = [\"Aims\"],\n",
        "    keep_columns = [\"itr\"],\n",
        "    n_jobs=multiprocessing.cpu_count()\n",
        "    )\n",
        "\n",
        "data_train['Label'] = labelling_data(data_train[\"itr\"], data_aims[\"itr\"])\n",
        "data_validate['Label'] = labelling_data(data_validate[\"itr\"], data_aims[\"itr\"])\n",
        "data_test['Label'] = labelling_data(data_test[\"itr\"], data_aims[\"itr\"])\n",
        "\n",
        "\n",
        "preprocessed_data_path = checkpoint_path + 'preprocessed_data/'\n",
        "if not os.path.exists(preprocessed_data_path):\n",
        "    os.makedirs(preprocessed_data_path)\n",
        "\n",
        "data_train.to_csv(preprocessed_data_path + \"01_train.csv\", index=False)\n",
        "data_validate.to_csv(preprocessed_data_path + \"01_validate.csv\", index=False)\n",
        "data_test.to_csv(preprocessed_data_path + \"01_test.csv\", index=False)\n",
        "data_aims.to_csv(preprocessed_data_path + \"01_aims.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
