{"cells":[{"cell_type":"markdown","metadata":{"id":"1mnwDh5YzBL0"},"source":["# Package preparation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3dkfofXmfSIH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409185141,"user_tz":-420,"elapsed":13527,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"d226b562-4d26-4314-b950-d2092ea8bea0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers==4.30 in /opt/conda/lib/python3.10/site-packages (4.30.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2024.5.15)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (2.32.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30) (2024.2.2)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.2.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.11.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (23.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\n","Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.2.1)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.30.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.31.0)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\n","Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.3)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.2.1)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers==4.30\n","!pip install datasets # huggingface dataset\n","!pip install bitsandbytes\n","!pip install peft\n","!pip install accelerate"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9YwQ8A_fe3v7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409188418,"user_tz":-420,"elapsed":3276,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"fc1e2a4e-6fc0-4105-bab9-0ecb11141774"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nvidia-smi in /opt/conda/lib/python3.10/site-packages (0.1.3)\r\n","Requirement already satisfied: numpy>=1.16.2 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.26.3)\r\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (1.16.0)\r\n","Requirement already satisfied: sorcery>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (0.2.2)\r\n","Requirement already satisfied: pytest>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from nvidia-smi) (8.2.2)\n","Requirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (23.1)\n","Requirement already satisfied: pluggy<2.0,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (1.2.0)\n","Requirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest>=4.3.1->nvidia-smi) (2.0.1)\n","Requirement already satisfied: executing in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (0.8.3)\n","Requirement already satisfied: littleutils>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (0.2.2)\n","Requirement already satisfied: asttokens in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (2.0.5)\n","Requirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from sorcery>=0.1.0->nvidia-smi) (1.16.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mFri Jun 14 23:53:06 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla P100-SXM2-16GB            On | 00000000:1C:00.0 Off |                    0 |\n","| N/A   61C    P0               60W / 300W|      4MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n","\n"]}],"source":["\n","! pip install nvidia-smi\n","import subprocess\n","if __name__ == \"__main__\":\n","    print(subprocess.check_output(['nvidia-smi']).decode('utf-8'))"]},{"cell_type":"markdown","metadata":{"id":"zDJ6A4QGzBL4"},"source":["## Import necessary packages"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DHO79vXRzzUW","executionInfo":{"status":"ok","timestamp":1718409193675,"user_tz":-420,"elapsed":5257,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","from typing import List\n","from numpy import ndarray\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import Tensor\n","import bitsandbytes as bnb\n","from datasets import load_dataset\n","import transformers\n","from transformers import Trainer\n","import torch.distributed as dist\n","\n","\n","\n","from transformers import BitsAndBytesConfig\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, AutoModel\n","from transformers import set_seed\n","from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPoolingAndCrossAttentions\n","from tqdm.notebook import trange, tqdm\n","\n","from peft import (\n","    prepare_model_for_kbit_training,\n","    LoraConfig,\n","    get_peft_model,\n","    get_peft_model_state_dict,\n","    set_peft_model_state_dict,\n",")\n","\n","from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n","from transformers.utils import PaddingStrategy\n","from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n","import numpy as np\n","from dataclasses import dataclass"]},{"cell_type":"markdown","metadata":{"id":"bc6dY28tzBL5"},"source":["## Some useful functions"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"1NJ2RN7LfAMv","executionInfo":{"status":"ok","timestamp":1718409193677,"user_tz":-420,"elapsed":1,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["def batch2device(batch, device):\n","    \"\"\"\n","    Transfer batch of training to GPU/CPU\n","    Args:\n","        batch: Dict[str, Tensor], represent for transformer input (input_ids, attention_mask)\n","        device: torch.device, GPU or CPU\n","    \"\"\"\n","    for key, value in batch.items():\n","        batch[key] = batch[key].to(device)\n","    return batch\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","# GPU accelerator\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"markdown","metadata":{"id":"JLtds5Z2zBL6"},"source":["# Data preparation"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"hDtULM23fCnm","executionInfo":{"status":"ok","timestamp":1718409193678,"user_tz":-420,"elapsed":1,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["checkpoint_path = \"Paper-submission/\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"IEtW5FCKfD6M","executionInfo":{"status":"ok","timestamp":1718409198567,"user_tz":-420,"elapsed":4889,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["data_train = pd.read_csv(checkpoint_path + \"01_train.csv\", encoding = \"ISO-8859-1\")\n","data_validate = pd.read_csv(checkpoint_path + \"01_validate.csv\", encoding = \"ISO-8859-1\")\n","data_test = pd.read_csv(checkpoint_path + \"01_test.csv\", encoding = \"ISO-8859-1\")\n","data_aims = pd.read_csv(checkpoint_path + \"01_aims.csv\", encoding = \"ISO-8859-1\")\n","\n","data_train.fillna(\"\", inplace=True)\n","data_validate.fillna(\"\", inplace=True)\n","data_test.fillna(\"\", inplace=True)\n","data_aims.fillna(\"\", inplace=True)\n","\n","n_classes = len(data_aims)"]},{"cell_type":"markdown","metadata":{"id":"Opss4cYHzBL7"},"source":["## Feature selection"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"A-gaX5tGfQrJ","executionInfo":{"status":"ok","timestamp":1718409199611,"user_tz":-420,"elapsed":419,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["X_train = (\n","    data_train['Title']\n","    + \" \" + data_train['Abstract']\n","    + \" \" + data_train['Keywords']\n","    ).tolist()\n","X_valid = (\n","    data_validate['Title']\n","    + \" \" + data_validate['Abstract']\n","    + \" \" + data_validate['Keywords']\n","    ).tolist()\n","X_test = (\n","    data_test['Title']\n","    + \" \" + data_test['Abstract']\n","    + \" \" + data_test['Keywords']\n","    ).tolist()\n","\n","X_aims = data_aims[\"Aims\"].tolist()\n","\n","Y_train = data_train['Label'].tolist()\n","Y_validate = data_validate['Label'].tolist()\n","Y_test = data_test['Label'].tolist()"]},{"cell_type":"markdown","metadata":{"id":"dBCJ9alCzBL7"},"source":["## Tokenization"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"laLk_ODBhiNN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409199825,"user_tz":-420,"elapsed":213,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"d6c7dd9a-7ec6-42f7-810b-bbce0124fba6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HLFYv0gRfbVC","executionInfo":{"status":"ok","timestamp":1718409305503,"user_tz":-420,"elapsed":105677,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["train_encodings = tokenizer(\n","    X_train,\n","    truncation=True,\n","    padding=\"max_length\",\n","    max_length=300,\n","    return_tensors=\"pt\"\n",")\n","valid_encodings = tokenizer(\n","    X_valid,\n","    truncation=True,\n","    padding=\"max_length\",\n","    max_length=300,\n","    return_tensors=\"pt\"\n",")\n","test_encodings = tokenizer(\n","    X_test,\n","    truncation=True,\n","    padding=\"max_length\",\n","    max_length=300,\n","    return_tensors=\"pt\"\n",")\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"kr_wSNBm6q6K","executionInfo":{"status":"ok","timestamp":1718409305505,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        x = {\n","            key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n","        }\n","        y = torch.tensor(self.labels[idx])\n","        return x, y\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"dWmwnGJR83uZ","executionInfo":{"status":"ok","timestamp":1718409305506,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["# Dataset\n","train_dataset = Dataset(train_encodings, Y_train)\n","valid_dataset = Dataset(valid_encodings, Y_validate)\n","test_dataset = Dataset(test_encodings, Y_test)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-7hsQoA737jc","executionInfo":{"status":"ok","timestamp":1718409305507,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["# Data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                         batch_size=16,\n","                                         shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,\n","                                         batch_size=8,\n","                                         shuffle=True)\n","test_loader = torch.utils.data.DataLoader(test_dataset,\n","                                         batch_size=8,\n","                                         shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"zxVq_W0azBL8"},"source":["# Model definition"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"mX7KdhR4NmPX","executionInfo":{"status":"ok","timestamp":1718409305508,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["class MLPLayer(nn.Module):\n","    \"\"\"\n","    Head for getting sentence representations over RoBERTa/BERT's CLS representation.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size * 2, config.hidden_size)\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, features, **kwargs):\n","        x = self.dense(features)\n","        x = self.activation(x)\n","\n","        return x\n","\n","class Similarity(nn.Module):\n","    \"\"\"\n","    Dot product or cosine similarity\n","    \"\"\"\n","\n","    def __init__(self, temp):\n","        super().__init__()\n","        self.temp = temp\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","\n","    def forward(self, x, y):\n","        return self.cos(x, y) / self.temp\n","\n","class AttentionLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.key = nn.Sequential(\n","            nn.Linear(config.hidden_size, config.hidden_size, bias=False),\n","            nn.RReLU()\n","        )\n","\n","        self.query = nn.Sequential(\n","            nn.Linear(config.hidden_size, config.hidden_size, bias=False),\n","            nn.RReLU()\n","        )\n","\n","        self.value = nn.Sequential(\n","            nn.Linear(config.hidden_size, config.hidden_size, bias=False),\n","            nn.RReLU()\n","        )\n","\n","    def forward(self, source_pooler_outputs, target_outputs):\n","        target_pooler_output_list = []\n","        for idx, source_pooler_output in enumerate(source_pooler_outputs):\n","            concated = target_outputs\n","            num_target_outputs = len(concated)\n","            concated = torch.stack(concated, dim=1)\n","\n","            K = self.key(concated)\n","            V = self.value(concated)\n","            Q = self.query(source_pooler_output)\n","\n","            score_list = torch.bmm(K, Q.unsqueeze(dim=-1)).squeeze(dim=-1)\n","            score_list /= num_target_outputs\n","            score_list = F.softmax(score_list, dim=-1)\n","            V = torch.mul(V, score_list.unsqueeze(dim=-1))\n","\n","            target_pooler_output = torch.sum(V, dim=1)\n","            target_pooler_output_list.append(target_pooler_output)\n","\n","        target_pooler_outputs = torch.stack(target_pooler_output_list, dim=1)\n","        target_pooler_output = torch.mean(target_pooler_outputs, dim=1)\n","\n","        pooler_output = torch.cat([source_pooler_outputs.pop(-1), target_pooler_output], dim=1)\n","        return pooler_output\n"]},{"cell_type":"markdown","metadata":{"id":"OkGw2jdjvQKm"},"source":["## Pooler layer"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"P1JAvMNV8rbW","executionInfo":{"status":"ok","timestamp":1718409305509,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["class Pooler(nn.Module):\n","    \"\"\"\n","    Parameter-free poolers to get the sentence embedding\n","    'cls': [CLS] representation with BERT/RoBERTa's MLP pooler.\n","    'cls_before_pooler': [CLS] representation without the original MLP pooler.\n","    'avg': average of the last layers' hidden states at each token.\n","    'avg_top2': average of the last two layers.\n","    'avg_first_last': average of the first and the last layers.\n","    \"\"\"\n","    def __init__(self, pooler_type):\n","        super().__init__()\n","        self.pooler_type = pooler_type\n","        assert self.pooler_type in [\"cls\", \"cls_before_pooler\", \"avg\", \"avg_top2\", \"avg_first_last\"], \"unrecognized pooling type %s\" % self.pooler_type\n","\n","    def forward(self, attention_mask, outputs):\n","        last_hidden = outputs.last_hidden_state\n","        hidden_states = outputs.hidden_states\n","\n","        if self.pooler_type in ['cls_before_pooler', 'cls']:\n","            source_pooled_result = [hidden[:, 0] for hidden in hidden_states]\n","            target_pooled_result = [((hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)) for hidden in hidden_states]\n","            return source_pooled_result, target_pooled_result\n","        elif self.pooler_type == \"avg\":\n","            return ((last_hidden * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1))\n","        elif self.pooler_type == \"avg_first_last\":\n","            first_hidden = hidden_states[0]\n","            last_hidden = hidden_states[-1]\n","            pooled_result = ((first_hidden + last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n","            return pooled_result\n","        elif self.pooler_type == \"avg_top2\":\n","            second_last_hidden = hidden_states[-2]\n","            last_hidden = hidden_states[-1]\n","            pooled_result = ((last_hidden + second_last_hidden) / 2.0 * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(-1).unsqueeze(-1)\n","            return pooled_result\n","        else:\n","            raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"MyvVPeuCvQKn"},"source":["# Sentence Embedder"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"7HG4b4GT8oCz","executionInfo":{"status":"ok","timestamp":1718409305524,"user_tz":-420,"elapsed":9,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["class ModelForCL(nn.Module):\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, model, model_name_or_path, pooler_type):\n","        super(ModelForCL, self).__init__()\n","        self.roberta = model\n","        self.pooler_type = pooler_type\n","        self.pooler = Pooler(self.pooler_type)\n","\n","        self.config  = AutoConfig.from_pretrained(model_name_or_path)\n","        self.attn = AttentionLayer(self.config)\n","        self.mlp = MLPLayer(self.config)\n","\n","\n","    def forward(self,\n","      input_ids=None,\n","      attention_mask=None,\n","      token_type_ids=None,\n","      position_ids=None,\n","      head_mask=None,\n","      inputs_embeds=None,\n","      labels=None,\n","      output_attentions=None,\n","      output_hidden_states=None,\n","      return_dict=None,\n","      sent_emb=False,\n","      mlm_input_ids=None,\n","      mlm_labels=None\n","      ):\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=True if self.pooler_type in ['cls', 'avg_top2', 'avg_first_last'] else False,\n","            return_dict=True,\n","        )\n","\n","        source_pooler_output, target_pooler_output = self.pooler(attention_mask, outputs)\n","        pooler_output = self.attn(source_pooler_output, target_pooler_output)\n","        pooler_output = self.mlp(pooler_output)\n","\n","\n","        return  BaseModelOutputWithPoolingAndCrossAttentions(\n","            pooler_output=pooler_output,\n","            last_hidden_state=outputs.last_hidden_state,\n","            hidden_states=outputs.hidden_states\n","        )\n","    def encode(self, sentences: Union[str, List[str]],\n","               batch_size: int = 8,\n","               show_progress_bar: bool = None,\n","               convert_to_numpy: bool = True,\n","               convert_to_tensor: bool = False,\n","               device: str = None) -> Union[List[Tensor], ndarray, Tensor]:\n","        self.eval()\n","\n","        if convert_to_tensor:\n","            convert_to_numpy = False\n","\n","        input_was_string = False\n","\n","        if isinstance(sentences, str) or not hasattr(sentences, '__len__'): #Cast an individual sentence to a list with length 1\n","            sentences = [sentences]\n","            input_was_string = True\n","\n","        if device is None:\n","            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","        self.to(device)\n","\n","        all_embeddings = []\n","        for start_index in trange(0, len(sentences), batch_size, desc=\"Batches\", disable=not show_progress_bar):\n","            sentence_batch = sentences[start_index: start_index+batch_size]\n","            features = tokenizer(sentence_batch,\n","                       padding='max_length',\n","                       truncation=True,\n","                       max_length=300,\n","                       return_tensors='pt').to(device)\n","\n","            with torch.no_grad():\n","                out_features = self.forward(**features)\n","                embeddings = []\n","                # gather the embedding vectors\n","                for row in out_features.pooler_output:\n","                    embeddings.append(row.cpu())\n","                all_embeddings.extend(embeddings)\n","        if convert_to_tensor:\n","            all_embeddings = torch.vstack(all_embeddings)\n","        elif convert_to_numpy:\n","            all_embeddings = np.asarray([emb.numpy() for emb in all_embeddings])\n","\n","        if input_was_string:\n","            all_embeddings = all_embeddings[0]\n","        return all_embeddings"]},{"cell_type":"markdown","metadata":{"id":"a-k9_ZQXzBL9"},"source":["## Load fine-tuned LM"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"CrB0d4rgfl26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409308649,"user_tz":-420,"elapsed":3120,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"70e7b6c0-9a77-472f-d590-b3a04cb61913"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["['query', 'key', 'dense', 'value']\n","trainable params: 1,339,392 || all params: 125,985,024 || trainable%: 1.0631\n"]}],"source":["from transformers import BitsAndBytesConfig\n","model = AutoModel.from_pretrained(\n","    'roberta-base',\n","    load_in_4bit=True,\n","    config=None,\n","    quantization_config=BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        llm_int8_threshold=6.0,\n","        llm_int8_has_fp16_weight=False,\n","        bnb_4bit_compute_dtype=torch.float32,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type='nf4',\n","    ),\n","    torch_dtype=torch.float32\n",")\n","\n","model = prepare_model_for_kbit_training(model)\n","def find_all_linear_names(model):\n","    cls = bnb.nn.Linear4bit\n","    lora_module_names = set()\n","    for name, module in model.named_modules():\n","        if isinstance(module, cls):\n","            names = name.split('.')\n","            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","\n","    return list(lora_module_names)\n","target_modules = find_all_linear_names(model)\n","print(target_modules)\n","\n","config = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=target_modules,\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=\"classification\"\n","    )\n","\n","model = get_peft_model(model, config)\n","from peft.tuners.lora import LoraLayer\n","for name, module in model.named_modules():\n","    if isinstance(module, LoraLayer):\n","        #module = module.to(torch.bfloat16)\n","        module = module.to(torch.float32)\n","    if 'norm' in name:\n","        module = module.to(torch.float32)\n","    if 'lm_head' in name or 'embed_tokens' in name:\n","        if hasattr(module, 'weight'):\n","            #module = module.to(torch.bfloat16)\n","            module = module.to(torch.float32)\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{"id":"nYN3sWYdtm3J"},"source":["# Training"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"X9-mqS6kN_U8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409309391,"user_tz":-420,"elapsed":741,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"330c33e0-0f86-45b1-94ca-054b3e991157"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":17}],"source":["# Fine-tuned LM checkpoint (by contrastive learning)\n","checkpoint_cl = torch.load(checkpoint_path + \"saved_model/Epoch_09 SupCL-RoBERTa.pth\")\n","\n","model_args = {\n","    'model': model,\n","    'model_name_or_path': 'roberta-base',\n","    'pooler_type': 'cls'\n","}\n","base_model = ModelForCL(**model_args)\n","base_model.load_state_dict(checkpoint_cl[\"model_state_dict\"])"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"OSydTiZ4fn4R","executionInfo":{"status":"ok","timestamp":1718409309392,"user_tz":-420,"elapsed":1,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["class NoAim_Classifier(nn.Module):\n","    def __init__(self, base_model, num_classes):\n","        super(NoAim_Classifier, self).__init__()\n","        self.base_model = base_model\n","        self.linear1_1 = nn.Linear(768, 512)\n","        self.act1_1 = nn.ReLU()\n","        self.drop1_1 = nn.Dropout(0.1)\n","\n","        self.linear1_2 = nn.Linear(512, num_classes)\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, inputs_tak):\n","        '''\n","        Args:\n","            inputs_tak: (dict) batch of TAK samples, shape as [bs, n_samples, encoding_dim]\n","            inputs_aims: (tensor) batch of aims embeddings taken by cls tokens, shape as [bs, n_samples, hidden_size]\n","        '''\n","        output_tak = self.base_model(**inputs_tak)\n","        x = output_tak.last_hidden_state[:,0,:] # cls tokens\n","\n","        x = self.linear1_1(x)\n","        x = self.act1_1(x)\n","        x = self.drop1_1(x)\n","\n","        x = self.linear1_2(x)\n","        return self.logsoftmax(x)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"uNHqajOAfz-J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718409309396,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"a808b013-1f6f-45a6-bed4-1b8910a495fa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["NoAim_Classifier(\n","  (base_model): ModelForCL(\n","    (roberta): PeftModel(\n","      (base_model): LoraModel(\n","        (model): RobertaModel(\n","          (embeddings): RobertaEmbeddings(\n","            (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","            (position_embeddings): Embedding(514, 768, padding_idx=1)\n","            (token_type_embeddings): Embedding(1, 768)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (encoder): RobertaEncoder(\n","            (layer): ModuleList(\n","              (0-11): 12 x RobertaLayer(\n","                (attention): RobertaAttention(\n","                  (self): RobertaSelfAttention(\n","                    (query): lora.Linear4bit(\n","                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.1, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=768, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=768, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                    )\n","                    (key): lora.Linear4bit(\n","                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.1, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=768, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=768, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                    )\n","                    (value): lora.Linear4bit(\n","                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.1, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=768, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=768, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                    )\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                  (output): RobertaSelfOutput(\n","                    (dense): lora.Linear4bit(\n","                      (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n","                      (lora_dropout): ModuleDict(\n","                        (default): Dropout(p=0.1, inplace=False)\n","                      )\n","                      (lora_A): ModuleDict(\n","                        (default): Linear(in_features=768, out_features=8, bias=False)\n","                      )\n","                      (lora_B): ModuleDict(\n","                        (default): Linear(in_features=8, out_features=768, bias=False)\n","                      )\n","                      (lora_embedding_A): ParameterDict()\n","                      (lora_embedding_B): ParameterDict()\n","                    )\n","                    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                    (dropout): Dropout(p=0.1, inplace=False)\n","                  )\n","                )\n","                (intermediate): RobertaIntermediate(\n","                  (dense): lora.Linear4bit(\n","                    (base_layer): Linear4bit(in_features=768, out_features=3072, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=3072, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (intermediate_act_fn): GELUActivation()\n","                )\n","                (output): RobertaOutput(\n","                  (dense): lora.Linear4bit(\n","                    (base_layer): Linear4bit(in_features=3072, out_features=768, bias=True)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=3072, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                )\n","              )\n","            )\n","          )\n","          (pooler): RobertaPooler(\n","            (dense): lora.Linear4bit(\n","              (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n","              (lora_dropout): ModuleDict(\n","                (default): Dropout(p=0.1, inplace=False)\n","              )\n","              (lora_A): ModuleDict(\n","                (default): Linear(in_features=768, out_features=8, bias=False)\n","              )\n","              (lora_B): ModuleDict(\n","                (default): Linear(in_features=8, out_features=768, bias=False)\n","              )\n","              (lora_embedding_A): ParameterDict()\n","              (lora_embedding_B): ParameterDict()\n","            )\n","            (activation): Tanh()\n","          )\n","        )\n","      )\n","    )\n","    (pooler): Pooler()\n","    (attn): AttentionLayer(\n","      (key): Sequential(\n","        (0): Linear(in_features=768, out_features=768, bias=False)\n","        (1): RReLU(lower=0.125, upper=0.3333333333333333)\n","      )\n","      (query): Sequential(\n","        (0): Linear(in_features=768, out_features=768, bias=False)\n","        (1): RReLU(lower=0.125, upper=0.3333333333333333)\n","      )\n","      (value): Sequential(\n","        (0): Linear(in_features=768, out_features=768, bias=False)\n","        (1): RReLU(lower=0.125, upper=0.3333333333333333)\n","      )\n","    )\n","    (mlp): MLPLayer(\n","      (dense): Linear(in_features=1536, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (linear1_1): Linear(in_features=768, out_features=512, bias=True)\n","  (act1_1): ReLU()\n","  (drop1_1): Dropout(p=0.1, inplace=False)\n","  (linear1_2): Linear(in_features=512, out_features=351, bias=True)\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"metadata":{},"execution_count":19}],"source":["# load checkpoint and continue training\n","model = NoAim_Classifier(base_model, n_classes)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"TN1-4jJszBL-"},"source":["## Optimizer and Loss function"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"5TZqFHAcf4mK","executionInfo":{"status":"ok","timestamp":1718409309397,"user_tz":-420,"elapsed":1,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["# Optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.96)\n","\n","# Loss function\n","loss_fn = nn.NLLLoss().to(device)"]},{"cell_type":"markdown","metadata":{"id":"Q4Kd7JXEzBL-"},"source":["## Training settings"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"zbkFjLPHf5uv","executionInfo":{"status":"ok","timestamp":1718409309398,"user_tz":-420,"elapsed":0,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[],"source":["max_epochs = 10\n","topks = [1, 3, 5, 10]\n","history = {\n","    \"train_loss\": [],\n","    \"val_loss\": [],\n","    \"train_acc@k\": [],\n","    \"val_acc@k\": [],\n","}\n","min_valid_loss = np.inf"]},{"cell_type":"markdown","metadata":{"id":"GXQWTIVLzBL-"},"source":["## Training loop"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"xGPBA4cYRqFW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718465904149,"user_tz":-420,"elapsed":419,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"outputId":"06428360-cb20-4d24-c9dd-154c8fd11ac0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:09 Roberta_TAK.pth found. Loading...\n","Model loaded successfully.\n","\tTraining loss: 1.2323567429371127\n","\tValidating loss: 1.4367562701788994\n","\n","\n","\tTrain accuracy@1: 0.5675908513426992\n","\tTrain accuracy@3: 0.8525125956616619\n","\tTrain accuracy@5: 0.9204034634298414\n","\tTrain accuracy@10: 0.9721839519705548\n","\n","\n","\tValidate accuracy@1: 0.5116601804084834\n","\tValidate accuracy@3: 0.816665158234531\n","\tValidate accuracy@5: 0.8938365462937823\n","\tValidate accuracy@10: 0.9543548435755875\n"]}],"source":["checkpoint_dir = checkpoint_path + \"weight/tak/\"\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","checkpoints = os.listdir(checkpoint_dir)\n","\n","if len(checkpoints)!=0:\n","    checkpoint_name = checkpoints[-1]\n","\n","    print(f\"{checkpoint_name} found. Loading...\")\n","    checkpoint_cl = torch.load(checkpoint_dir + checkpoint_name)\n","    model.load_state_dict(checkpoint_cl[\"model_state_dict\"])\n","    optimizer.load_state_dict(checkpoint_cl['optimizer_state_dict'])\n","    history = checkpoint_cl['history']\n","    epoch = checkpoint_cl[\"epoch\"]\n","    train_loss = history[\"train_loss\"][-1]\n","    min_valid_loss = history[\"val_loss\"][-1]\n","    print(\"Model loaded successfully.\")\n","    print(\"\\tTraining loss: {}\".format(train_loss))\n","    print(\"\\tValidating loss: {}\".format(min_valid_loss))\n","    print(\"\\n\")\n","    for k in topks:\n","        print(\"\\tTrain accuracy@{}: {}\".format(k, history[\"train_acc@k\"][-1][k]))\n","    print(\"\\n\")\n","    for k in topks:\n","        print(\"\\tValidate accuracy@{}: {}\".format(k, history[\"val_acc@k\"][-1][k]))\n","    epoch+=1\n","else:\n","    epoch = 0\n","    print(f\"Starting training from epoch {epoch}\")\n","    min_loss = np.inf"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"9PWI1t-nf7Ev","collapsed":true,"colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["bc7ac31d306f48499469f3d47af2d3e0","639e80ea6b544a1d928b4159dbd2bbe6","5a334f6198584f748ac475979791be2f","a20ac25de8fd40b19fa28dee386c4eb5","1b8425b2b7524925b146f5f600363bb1","cf7a985a49d2432eaf2d81e6bc02d613","ca3374ad17284da695ca631fc66ab576","3267112d109a44149b4b0e36071bf43a","104cc266ff604bddbc0bc3d88ca3276d","b93fee5e384a463492dc7a2581a33ffe","15d8415aa6664153a184b3c6f1488fd1","3be26dc355f44dfaaefe06b1ab8a870b","5db1f1a61a804f79991d06a8e8d76fff","285eb815eef14007929023af91102372","61e9ded13c3648ee94c43587445b53fb","e4dda6bcf6e64cf9bf84da3414a74112","e4df709b232141549d4d35dbb7f83f7b","d575a18a177c4cfdbbced5dc725fb7b7","7555350d3e61456c87f8d7c5056bf72c","b4f4bb6aa1a8414399cebf9b32a68d9e","60a3e628ea424ea0af68ab4a0d57d768","8d78569ebcf24a9baab8246be736145d","78915c109d964a18a408cad9c1c1fac6","7746ef0ac9334205b379f66798abe06c","144ff5120a124b97bcc30f1bfcac87e3","3125223606744ebd8833e95e7fb0cf0c","5a1446dfd7a84e9d97e34767f2592f77","79881a6ef3754629b02818e152c9d7c7","b89e9ba70b2a4c08a49f0136e2f536ed","a3a11c5a984041be89885e55b9bfa15c","a70c4ade37e44c49b4d794fa6170c613","3689321d07704b62adca10d65a93aa37","124bb158d568485dabf68601ac56b20b","2f162c0645964437a2ec79ec28ff09ed","caeaea49c6da4b058f0fd36e66ffda68","61dee441d9f9469ca0168dad063ccb2b","6bba0ee9a7cc4a9bac86671d2c7d6413","b149d3e506d64de5b9ea2cd49a4c6ace","226198da96b94b6a8bd02cd833e8215f","0379134f57474b9d94248f4b53329f2c","1c3b651a20764616b23d342b96fe8ce3","939351c8e72641119c8b16d3e15a3cf7","dbcad531674f484493f41110b55889e9","073289e8cc6b473e8ee737eae5aa8a5b"]},"outputId":"0964534e-92ed-4255-cd78-4b4ac1cf836b","executionInfo":{"status":"ok","timestamp":1718460576124,"user_tz":-420,"elapsed":24211566,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/18645 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc7ac31d306f48499469f3d47af2d3e0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_3569/3643185941.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n","/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4144 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be26dc355f44dfaaefe06b1ab8a870b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[">> Epoch 9 \t\t Training Loss: 1.173226931335524 \t\t Validation Loss: 1.4531630800576687\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/18645 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78915c109d964a18a408cad9c1c1fac6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4144 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f162c0645964437a2ec79ec28ff09ed"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":[">> Epoch 10 \t\t Training Loss: 1.1487185428404048 \t\t Validation Loss: 1.4285600668099856\n","Validation Loss Decreased(1.436756--->1.428560) \t Saving The Model\n"]}],"source":["for epoch in range(epoch,max_epochs+1):\n","    train_loss = 0.0\n","    train_loop = tqdm(train_loader, leave=True)\n","    batch_train_accuracy = {k: 0 for k in topks}\n","    batch_valid_accuracy = {k: 0 for k in topks}\n","    num_correct_at_k = {\n","        \"train\": {k: 0 for k in topks},\n","        \"val\": {k: 0 for k in topks}\n","    }\n","    # Training\n","    model.train()\n","\n","    for features, labels in train_loop:\n","\n","        # Transfer Data to GPU if available\n","        if torch.cuda.is_available():\n","            features, labels = batch2device(features, device), labels.to(device)\n","        # forward pass\n","        logits = model(features)\n","        # Clear the gradients\n","        optimizer.zero_grad()\n","        # Find the Loss\n","        loss = loss_fn(logits, labels)\n","        # Calculate gradients\n","        loss.backward()\n","        # Update Weights\n","        optimizer.step()\n","        # Calculate accuracy\n","        probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","        for k in topks:\n","            batch_num_correct = 0\n","            nPoints = len(labels)\n","            for i in range(nPoints):\n","                if labels[i] in probs_des[i, 0:k]:\n","                    batch_num_correct += 1\n","                    num_correct_at_k[\"train\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","            batch_train_accuracy[k] = batch_num_correct / nPoints\n","        # Calculate Loss\n","        train_loss += loss.item()\n","        train_loop.set_description('Epoch: {0} - lr: {1}, Training'.format(epoch, optimizer.param_groups[0]['lr']))\n","        train_loop.set_postfix(train_loss=loss.item(),\n","                               top01=batch_train_accuracy[1],\n","                               top03=batch_train_accuracy[3],\n","                               top05=batch_train_accuracy[5],\n","                               top10=batch_train_accuracy[10])\n","    train_loss = train_loss/len(train_loader)\n","    history[\"train_loss\"].append(train_loss)\n","    history[\"train_acc@k\"].append(\n","        {k: val/len(X_train) for k, val in num_correct_at_k[\"train\"].items()}\n","    )\n","\n","    # Validation\n","    valid_loss = 0.0\n","    valid_loop = tqdm(valid_loader, leave=True)\n","    with torch.no_grad():\n","        model.eval()\n","        # Transfer Data to GPU if available\n","        for features, labels in valid_loop:\n","\n","            if torch.cuda.is_available():\n","                features, labels = batch2device(features, device), labels.to(device)\n","            # Forward pass\n","            logits = model(features)\n","\n","            # Find the Loss\n","            loss = loss_fn(logits, labels)\n","            # Calculate accuracy\n","            probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","            for k in topks:\n","                num_correct = 0\n","                nPoints = len(labels)\n","                for i in range(nPoints):\n","                    if labels[i] in probs_des[i, 0:k]:\n","                        num_correct += 1\n","                        num_correct_at_k[\"val\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","                batch_valid_accuracy[k] = num_correct / nPoints\n","            # Calculate Loss\n","            valid_loss += loss.item()\n","            valid_loop.set_description('Epoch: {0} - lr: {1}, Validating'.format(epoch, optimizer.param_groups[0]['lr']))\n","            valid_loop.set_postfix(val_loss=loss.item(),\n","                                val_top01=batch_valid_accuracy[1],\n","                                val_top03=batch_valid_accuracy[3],\n","                                val_top05=batch_valid_accuracy[5],\n","                                val_top10=batch_valid_accuracy[10])\n","        valid_loss = valid_loss/len(valid_loader)\n","        history[\"val_loss\"].append(valid_loss)\n","        history[\"val_acc@k\"].append(\n","            {k: val/len(X_valid) for k, val in num_correct_at_k[\"val\"].items()}\n","        )\n","        print(f'>> Epoch {epoch} \\t\\t Training Loss: {train_loss} \\t\\t Validation Loss: {valid_loss}')\n","        # lr_scheduler.step()\n","\n","        if min_valid_loss > valid_loss:\n","            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n","            min_valid_loss = valid_loss\n","\n","            saved_path = checkpoint_path + \"weight/tak/\"\n","            # Saving State Dict\n","            torch.save(\n","                {\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'history': history,\n","                    'epoch': epoch\n","                }, saved_path + \"Epoch:{:0>2} Roberta_TAK.pth\".format(epoch)\n","            )"]},{"cell_type":"markdown","metadata":{"id":"l7wNgQATrBr6"},"source":["# Testing"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"SOP0nAFxtq5V","executionInfo":{"status":"ok","timestamp":1718465997221,"user_tz":-420,"elapsed":220,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad1c159d-dd25-4b3a-9aa1-ccd9096f1866"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:09 Roberta_TAK.pth\n"]}],"source":["# load checkpoint and testing\n","checkpoints = os.listdir(checkpoint_dir)\n","checkpoint_name = checkpoints[-1]\n","print(checkpoint_name)\n","checkpoint = torch.load(checkpoint_dir + checkpoint_name)\n","\n","\n","model = NoAim_Classifier(base_model, n_classes)\n","model.load_state_dict(checkpoint[\"model_state_dict\"])\n","model.to(device)\n","\n","history = checkpoint['history']"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"Vr3eDjKAf90e","executionInfo":{"status":"ok","timestamp":1718467051258,"user_tz":-420,"elapsed":1052120,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"colab":{"base_uri":"https://localhost:8080/","height":73,"referenced_widgets":["a734885927114456a2f0d53c8fcdf8a7","ae6a98ceaf5541cc84ef669e9e114a54","3d6480efe8ee43279eb529358411c51b","9fcc0d7cbd934090bcac0951204e4481","2fb8ee299afa4a669d194faf0ec10835","049872be79a24e0b81eef5478b9dbbc3","8359f6e7033248bb92ec0ebdf6097f63","5a4ccfc985c1499e88c84f4f445c8f39","18925369188a4e90b186e52282e4ed5a","ce74033570ce49f2b68ab25964f2ac97","25f1119f25b84366b8fdb3bb97f05d1e"]},"outputId":"e00c2707-88ee-4b86-a5de-7fa5d2764941"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/10381 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a734885927114456a2f0d53c8fcdf8a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipykernel_3569/3643185941.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  key: torch.tensor(val[idx]) for key, val in self.encodings.items()\n"]}],"source":["# Loss function\n","loss_fn = nn.NLLLoss().to(device)\n","\n","# Test\n","topks = [1, 3, 5, 10]\n","num_correct_at_k = {}\n","test_loop = tqdm(test_loader, leave=True)\n","num_correct_at_k[\"test\"] = {k: 0 for k in topks}\n","batch_test_accuracy = {k: [] for k in topks}\n","history[\"test_acc@k\"] = []\n","history[\"test_loss\"] = []\n","test_loss = 0.0\n","\n","with torch.no_grad():\n","    model.eval()\n","    for features, labels in test_loop:\n","        # Transfer Data to GPU if available\n","        if torch.cuda.is_available():\n","            features, labels = batch2device(features, device), labels.to(device)\n","        logits = model(features)\n","        # Find the Loss\n","        loss = loss_fn(logits, labels)\n","        # Calculate accuracy\n","        probs_des = torch.argsort(torch.exp(logits), axis=1, descending=True)\n","        for k in topks:\n","            num_correct = 0\n","            nPoints = len(labels)\n","            for i in range(nPoints):\n","                if labels[i] in probs_des[i, 0:k]:\n","                    num_correct += 1\n","                    num_correct_at_k[\"test\"][k] += 1 # globally counting number of correct at each k's for whole valid set\n","            batch_test_accuracy[k] = num_correct / nPoints\n","        # Calculate Loss\n","        test_loss += loss.item()\n","        test_loop.set_description('Testing...')\n","        test_loop.set_postfix(test_loss=loss.item(),\n","                            test_top01=batch_test_accuracy[1],\n","                            test_top03=batch_test_accuracy[3],\n","                            test_top05=batch_test_accuracy[5],\n","                            test_top10=batch_test_accuracy[10])\n","    test_loss = test_loss/len(test_loader)\n","    history[\"test_loss\"].append(test_loss)\n","    history[\"test_acc@k\"].append(\n","        {k: val/len(X_test) for k, val in num_correct_at_k[\"test\"].items()}\n","    )"]},{"cell_type":"markdown","metadata":{"id":"lJ60IrJ4q9cG"},"source":["# Final results"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"DPF5969EqV6X","executionInfo":{"status":"ok","timestamp":1718467051263,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quang Đỉnh Nguyễn Đinh","userId":"04543532491475917377"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a723c88e-970b-4172-b971-e852dcb23c9c"},"outputs":[{"output_type":"stream","name":"stdout","text":[">> Final results (Best model): \n","\tTraining loss: 1.2323567429371127\n","\tValidating loss: 1.4367562701788994\n","\tTesting loss: 1.4441883676546863\n","\n","\n","\tTrain accuracy@1: 0.5675908513426992\n","\tTrain accuracy@3: 0.8525125956616619\n","\tTrain accuracy@5: 0.9204034634298414\n","\tTrain accuracy@10: 0.9721839519705548\n","\n","\n","\tValidate accuracy@1: 0.5116601804084834\n","\tValidate accuracy@3: 0.816665158234531\n","\tValidate accuracy@5: 0.8938365462937823\n","\tValidate accuracy@10: 0.9543548435755875\n","\n","\n","\tTest accuracy@1: 0.511583662460264\n","\tTest accuracy@3: 0.8153838743858973\n","\tTest accuracy@5: 0.8929414314613235\n","\tTest accuracy@10: 0.9553390810133898\n"]}],"source":["print(\">> Final results (Best model): \")\n","print(\"\\tTraining loss: {}\".format(history[\"train_loss\"][-1]))\n","print(\"\\tValidating loss: {}\".format(history[\"val_loss\"][-1]))\n","print(\"\\tTesting loss: {}\".format(history[\"test_loss\"][-1]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tTrain accuracy@{}: {}\".format(k, history[\"train_acc@k\"][-1][k]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tValidate accuracy@{}: {}\".format(k, history[\"val_acc@k\"][-1][k]))\n","print(\"\\n\")\n","for k in topks:\n","    print(\"\\tTest accuracy@{}: {}\".format(k, history[\"test_acc@k\"][-1][k]))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1h5O3dkWrICAYnpYBJvb7c_JrmCLz7BJX","timestamp":1716463202827}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bc7ac31d306f48499469f3d47af2d3e0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_639e80ea6b544a1d928b4159dbd2bbe6","IPY_MODEL_5a334f6198584f748ac475979791be2f","IPY_MODEL_a20ac25de8fd40b19fa28dee386c4eb5"],"layout":"IPY_MODEL_1b8425b2b7524925b146f5f600363bb1","tabbable":null,"tooltip":null}},"639e80ea6b544a1d928b4159dbd2bbe6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_cf7a985a49d2432eaf2d81e6bc02d613","placeholder":"​","style":"IPY_MODEL_ca3374ad17284da695ca631fc66ab576","tabbable":null,"tooltip":null,"value":"Epoch: 9 - lr: 5e-05, Training: 100%"}},"5a334f6198584f748ac475979791be2f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_3267112d109a44149b4b0e36071bf43a","max":18645,"min":0,"orientation":"horizontal","style":"IPY_MODEL_104cc266ff604bddbc0bc3d88ca3276d","tabbable":null,"tooltip":null,"value":18645}},"a20ac25de8fd40b19fa28dee386c4eb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b93fee5e384a463492dc7a2581a33ffe","placeholder":"​","style":"IPY_MODEL_15d8415aa6664153a184b3c6f1488fd1","tabbable":null,"tooltip":null,"value":" 18645/18645 [3:14:51&lt;00:00,  1.67it/s, top01=0.846, top03=0.923, top05=0.923, top10=1, train_loss=0.717]"}},"1b8425b2b7524925b146f5f600363bb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf7a985a49d2432eaf2d81e6bc02d613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca3374ad17284da695ca631fc66ab576":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3267112d109a44149b4b0e36071bf43a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"104cc266ff604bddbc0bc3d88ca3276d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b93fee5e384a463492dc7a2581a33ffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d8415aa6664153a184b3c6f1488fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"3be26dc355f44dfaaefe06b1ab8a870b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5db1f1a61a804f79991d06a8e8d76fff","IPY_MODEL_285eb815eef14007929023af91102372","IPY_MODEL_61e9ded13c3648ee94c43587445b53fb"],"layout":"IPY_MODEL_e4dda6bcf6e64cf9bf84da3414a74112","tabbable":null,"tooltip":null}},"5db1f1a61a804f79991d06a8e8d76fff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e4df709b232141549d4d35dbb7f83f7b","placeholder":"​","style":"IPY_MODEL_d575a18a177c4cfdbbced5dc725fb7b7","tabbable":null,"tooltip":null,"value":"Epoch: 9 - lr: 5e-05, Validating: 100%"}},"285eb815eef14007929023af91102372":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_7555350d3e61456c87f8d7c5056bf72c","max":4144,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4f4bb6aa1a8414399cebf9b32a68d9e","tabbable":null,"tooltip":null,"value":4144}},"61e9ded13c3648ee94c43587445b53fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_60a3e628ea424ea0af68ab4a0d57d768","placeholder":"​","style":"IPY_MODEL_8d78569ebcf24a9baab8246be736145d","tabbable":null,"tooltip":null,"value":" 4144/4144 [07:02&lt;00:00,  9.75it/s, val_loss=0.44, val_top01=0.667, val_top03=1, val_top05=1, val_top10=1]"}},"e4dda6bcf6e64cf9bf84da3414a74112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4df709b232141549d4d35dbb7f83f7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d575a18a177c4cfdbbced5dc725fb7b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"7555350d3e61456c87f8d7c5056bf72c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4f4bb6aa1a8414399cebf9b32a68d9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"60a3e628ea424ea0af68ab4a0d57d768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d78569ebcf24a9baab8246be736145d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"78915c109d964a18a408cad9c1c1fac6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7746ef0ac9334205b379f66798abe06c","IPY_MODEL_144ff5120a124b97bcc30f1bfcac87e3","IPY_MODEL_3125223606744ebd8833e95e7fb0cf0c"],"layout":"IPY_MODEL_5a1446dfd7a84e9d97e34767f2592f77","tabbable":null,"tooltip":null}},"7746ef0ac9334205b379f66798abe06c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_79881a6ef3754629b02818e152c9d7c7","placeholder":"​","style":"IPY_MODEL_b89e9ba70b2a4c08a49f0136e2f536ed","tabbable":null,"tooltip":null,"value":"Epoch: 10 - lr: 5e-05, Training: 100%"}},"144ff5120a124b97bcc30f1bfcac87e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_a3a11c5a984041be89885e55b9bfa15c","max":18645,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a70c4ade37e44c49b4d794fa6170c613","tabbable":null,"tooltip":null,"value":18645}},"3125223606744ebd8833e95e7fb0cf0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_3689321d07704b62adca10d65a93aa37","placeholder":"​","style":"IPY_MODEL_124bb158d568485dabf68601ac56b20b","tabbable":null,"tooltip":null,"value":" 18645/18645 [3:14:37&lt;00:00,  1.68it/s, top01=0.769, top03=1, top05=1, top10=1, train_loss=0.765]"}},"5a1446dfd7a84e9d97e34767f2592f77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79881a6ef3754629b02818e152c9d7c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89e9ba70b2a4c08a49f0136e2f536ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a3a11c5a984041be89885e55b9bfa15c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a70c4ade37e44c49b4d794fa6170c613":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3689321d07704b62adca10d65a93aa37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"124bb158d568485dabf68601ac56b20b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2f162c0645964437a2ec79ec28ff09ed":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_caeaea49c6da4b058f0fd36e66ffda68","IPY_MODEL_61dee441d9f9469ca0168dad063ccb2b","IPY_MODEL_6bba0ee9a7cc4a9bac86671d2c7d6413"],"layout":"IPY_MODEL_b149d3e506d64de5b9ea2cd49a4c6ace","tabbable":null,"tooltip":null}},"caeaea49c6da4b058f0fd36e66ffda68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_226198da96b94b6a8bd02cd833e8215f","placeholder":"​","style":"IPY_MODEL_0379134f57474b9d94248f4b53329f2c","tabbable":null,"tooltip":null,"value":"Epoch: 10 - lr: 5e-05, Validating: 100%"}},"61dee441d9f9469ca0168dad063ccb2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1c3b651a20764616b23d342b96fe8ce3","max":4144,"min":0,"orientation":"horizontal","style":"IPY_MODEL_939351c8e72641119c8b16d3e15a3cf7","tabbable":null,"tooltip":null,"value":4144}},"6bba0ee9a7cc4a9bac86671d2c7d6413":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dbcad531674f484493f41110b55889e9","placeholder":"​","style":"IPY_MODEL_073289e8cc6b473e8ee737eae5aa8a5b","tabbable":null,"tooltip":null,"value":" 4144/4144 [07:01&lt;00:00,  9.85it/s, val_loss=0.503, val_top01=1, val_top03=1, val_top05=1, val_top10=1]"}},"b149d3e506d64de5b9ea2cd49a4c6ace":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"226198da96b94b6a8bd02cd833e8215f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0379134f57474b9d94248f4b53329f2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1c3b651a20764616b23d342b96fe8ce3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939351c8e72641119c8b16d3e15a3cf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbcad531674f484493f41110b55889e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"073289e8cc6b473e8ee737eae5aa8a5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a734885927114456a2f0d53c8fcdf8a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae6a98ceaf5541cc84ef669e9e114a54","IPY_MODEL_3d6480efe8ee43279eb529358411c51b","IPY_MODEL_9fcc0d7cbd934090bcac0951204e4481"],"layout":"IPY_MODEL_2fb8ee299afa4a669d194faf0ec10835","tabbable":null,"tooltip":null}},"ae6a98ceaf5541cc84ef669e9e114a54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_049872be79a24e0b81eef5478b9dbbc3","placeholder":"​","style":"IPY_MODEL_8359f6e7033248bb92ec0ebdf6097f63","tabbable":null,"tooltip":null,"value":"Testing...: 100%"}},"3d6480efe8ee43279eb529358411c51b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_5a4ccfc985c1499e88c84f4f445c8f39","max":10381,"min":0,"orientation":"horizontal","style":"IPY_MODEL_18925369188a4e90b186e52282e4ed5a","tabbable":null,"tooltip":null,"value":10381}},"9fcc0d7cbd934090bcac0951204e4481":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_ce74033570ce49f2b68ab25964f2ac97","placeholder":"​","style":"IPY_MODEL_25f1119f25b84366b8fdb3bb97f05d1e","tabbable":null,"tooltip":null,"value":" 10381/10381 [17:32&lt;00:00,  9.64it/s, test_loss=1.24, test_top01=0.625, test_top03=0.875, test_top05=0.875, test_top10=1]"}},"2fb8ee299afa4a669d194faf0ec10835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"049872be79a24e0b81eef5478b9dbbc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8359f6e7033248bb92ec0ebdf6097f63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"5a4ccfc985c1499e88c84f4f445c8f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18925369188a4e90b186e52282e4ed5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce74033570ce49f2b68ab25964f2ac97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25f1119f25b84366b8fdb3bb97f05d1e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}}}},"nbformat":4,"nbformat_minor":0}